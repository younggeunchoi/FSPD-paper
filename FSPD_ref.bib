@article{Bickel2008,
author = {Bickel, Peter J. and Levina, Elizaveta},
doi = {10.1214/009053607000000758},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Bickel, Levina - 2008 - Regularized estimation of large covariance matrices.pdf:pdf},
issn = {0090-5364},
journal = {Ann. Stat.},
month = {feb},
number = {1},
pages = {199--227},
title = {{Regularized estimation of large covariance matrices}},
url = {http://projecteuclid.org/euclid.aos/1201877299},
volume = {36},
year = {2008}
}
@article{Bickel2008a,
author = {Bickel, Peter J. and Levina, Elizaveta},
doi = {10.1214/08-AOS600},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Bickel, Levina - 2008 - Covariance regularization by thresholding.pdf:pdf},
issn = {0090-5364},
journal = {Ann. Stat.},
month = {dec},
number = {6},
pages = {2577--2604},
title = {{Covariance regularization by thresholding}},
url = {http://projecteuclid.org/euclid.aos/1231165180},
volume = {36},
year = {2008}
}
@article{Bien2011,
abstract = {We suggest a method for estimating a covariance matrix on the basis of a sample of vectors drawn from a multivariate normal distribution. In particular, we penalize the likelihood with a lasso penalty on the entries of the covariance matrix. This penalty plays two important roles: it reduces the effective number of parameters, which is important even when the dimension of the vectors is smaller than the sample size since the number of parameters grows quadratically in the number of variables, and it produces an estimate which is sparse. In contrast to sparse inverse covariance estimation, our method's close relative, the sparsity attained here is in the covariance matrix itself rather than in the inverse matrix. Zeros in the covariance matrix correspond to marginal independencies; thus, our method performs model selection while providing a positive definite estimate of the covariance. The proposed penalized maximum likelihood problem is not convex, so we use a majorize-minimize approach in which we iteratively solve convex approximations to the original nonconvex problem. We discuss tuning parameter selection and demonstrate on a flow-cytometry dataset how our method produces an interpretable graphical display of the relationship between variables. We perform simulations that suggest that simple elementwise thresholding of the empirical covariance matrix is competitive with our method for identifying the sparsity structure. Additionally, we show how our method can be used to solve a previously studied special case in which a desired sparsity pattern is prespecified.},
author = {Bien, Jacob and Tibshirani, Robert J},
doi = {10.1093/biomet/asr054},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Bien, Tibshirani - 2011 - Sparse estimation of a covariance matrix.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = {dec},
number = {4},
pages = {807--820},
pmid = {23049130},
title = {{Sparse estimation of a covariance matrix}},
volume = {98},
year = {2011}
}
@article{Boyd2010,
abstract = {Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas–Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for 1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations},
author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
doi = {10.1561/2200000016},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Boyd et al. - 2010 - Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.pdf:pdf},
isbn = {1935823719358},
issn = {1935-8237},
journal = {Found. Trends Mach. Learn.},
number = {1},
pages = {1--122},
title = {{Distributed optimization and statistical learning via the alternating direction method of multipliers}},
url = {https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=8186925},
volume = {3},
year = {2010}
}
@article{Cai2011a,
author = {Cai, T. Tony and Low, Mark},
doi = {10.5705/ss.2013.279},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Low - 2011 - A framework for estimation of convex functions.pdf:pdf},
issn = {10170405},
journal = {Stat. Sinica},
pages = {423--456},
title = {{A framework for estimation of convex functions}},
url = {http://www3.stat.sinica.edu.tw/statistica/J25N2/J25N21/J25N21.html},
volume = {25},
year = {2015}
}
@article{Cai2016a,
abstract = {This is an expository paper that reviews recent developments on optimal estimation of structured high-dimensional covariance and precision matrices. Minimax rates of convergence for estimating several classes of structured covariance and precision matrices, including bandable, Toeplitz, and sparse covariance matrices as well as sparse precision matrices, are given under the spectral norm loss. Data-driven adaptive procedures for estimating various classes of matrices are presented. Some key technical tools including large deviation results and minimax lower bound arguments that are used in the theoretical analyses are discussed. In addition, estimation under other losses and a few related problems such as Gaussian graphical models, sparse principal component analysis, and hypothesis testing on the covariance structure are considered. Some open problems on estimating high-dimensional covariance and precision matrices and their functionals are also discussed.},
author = {Cai, T. Tony and Ren, Zhao and Zhou, Harrison H},
doi = {10.1214/15-EJS1081},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Ren, Zhou - 2016 - Estimating structured high-dimensional covariance and precision matrices Optimal rates and adaptive estimation.pdf:pdf},
issn = {1935-7524},
journal = {Electron. J. Stat.},
keywords = {adaptive estimation,banding,block thresholding,covariance matrix,factor model,frobenius norm,gaussian graphical model,hypothesis testing,minimax lower bound,norm,operator,optimal rate of convergence,precision matrix,schatten norm,spectral norm,tapering},
number = {1},
pages = {1--59},
title = {{Estimating structured high-dimensional covariance and precision matrices: Optimal rates and adaptive estimation}},
url = {http://projecteuclid.org/euclid.ejs/1455715952},
volume = {10},
year = {2016}
}
@article{Cai2012c,
author = {Cai, T. Tony and Yuan, Ming},
doi = {10.1214/12-AOS999},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Yuan - 2012 - Adaptive covariance matrix estimation through block thresholding.pdf:pdf},
issn = {0090-5364},
journal = {Ann. Stat.},
month = {aug},
number = {4},
pages = {2014--2042},
title = {{Adaptive covariance matrix estimation through block thresholding}},
url = {http://projecteuclid.org/euclid.aos/1351602535},
volume = {40},
year = {2012}
}
@article{Cai2010,
author = {Cai, T. Tony and Zhang, Cun-Hui and Zhou, Harrison H.},
doi = {10.1214/09-AOS752},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Zhang, Zhou - 2010 - Optimal rates of convergence for covariance matrix estimation.pdf:pdf},
issn = {0090-5364},
journal = {Ann. Stat.},
month = {aug},
number = {4},
pages = {2118--2144},
title = {{Optimal rates of convergence for covariance matrix estimation}},
url = {http://projecteuclid.org/euclid.aos/1278861244},
volume = {38},
year = {2010}
}
@article{Cai2012f,
author = {Cai, T. Tony and Zhou, Harrison H.},
doi = {10.1214/12-AOS998},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Zhou - 2012 - Optimal rates of convergence for sparse covariance matrix estimation.pdf:pdf;:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Zhou - 2012 - Optimal rates of convergence for sparse covariance matrix estimation(2).pdf:pdf},
issn = {0090-5364},
journal = {Ann. Stat.},
month = {oct},
number = {5},
pages = {2389--2420},
title = {{Optimal rates of convergence for sparse covariance matrix estimation}},
url = {http://projecteuclid.org/euclid.aos/1359987525},
volume = {40},
year = {2012}
}
@article{Cai2011b,
abstract = {In this article we consider estimation of sparse covariance matrices and propose a thresholding procedure that is adaptive to the variability of individual entries. The estimators are fully data-driven and demonstrate excellent performance both theoretically and numerically. It is shown that the estimators adaptively achieve the optimal rate of convergence over a large class of sparse covariance matrices under the spectral norm. In contrast, the commonly used universal thresholding estimators are shown to be suboptimal over the same parameter spaces. Support recovery is discussed as well. The adaptive thresholding estimators are easy to implement. The numerical performance of the estimators is studied using both simulated and real data. Simulation results demonstrate that the adaptive thresholding estimators uniformly outperform the universal thresholding estimators. The method is also illustrated in an analysis on a dataset from a small round blue-cell tumor microarray experiment. A supplement to this article presenting additional technical proofs is available online.},
author = {Cai, Tony and Liu, Weidong},
doi = {10.1198/jasa.2011.tm10560},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Liu - 2011 - Adaptive Thresholding for Sparse Covariance Matrix Estimation.pdf:pdf},
issn = {0162-1459},
journal = {J. Amer. Stat. Assoc.},
keywords = {Frobenius norm,Optimal rate of convergence,Spectral norm,Support recovery,Universal thresholding},
mendeley-tags = {Frobenius norm,Optimal rate of convergence,Spectral norm,Support recovery,Universal thresholding},
month = {jun},
number = {494},
pages = {672--684},
title = {{Adaptive thresholding for sparse covariance matrix estimation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10560},
volume = {106},
year = {2011}
}
@article{Cai2011,
abstract = {A constrained L1 minimization method is proposed for estimating a sparse inverse covariance matrix based on a sample of {\{}{\$}{\}}n{\{}{\$}{\}} iid {\{}{\$}{\}}p{\{}{\$}{\}}-variate random variables. The resulting estimator is shown to enjoy a number of desirable properties. In particular, it is shown that the rate of convergence between the estimator and the true {\{}{\$}{\}}s{\{}{\$}{\}}-sparse precision matrix under the spectral norm is {\{}{\$}{\}}s\backslashbackslashsqrt{\{}\backslash{\{}{\}}\backslashbackslashlog p/n{\{}\backslash{\}}{\}}{\{}{\$}{\}} when the population distribution has either exponential-type tails or polynomial-type tails. Convergence rates under the elementwise {\{}{\$}{\}}L{\{}{\_}{\}}{\{}{\{}{\}}\backslashbackslashinfty{\{}\backslash{\}}{\}}{\{}{\$}{\}} norm and Frobenius norm are also presented. In addition, graphical model selection is considered. The procedure is easily implementable by linear programming. Numerical performance of the estimator is investigated using both simulated and real data. In particular, the procedure is applied to analyze a breast cancer dataset. The procedure performs favorably in comparison to existing methods.},
archivePrefix = {arXiv},
arxivId = {1102.2233},
author = {Cai, Tony and Liu, Weidong and Luo, Xi},
doi = {10.1198/jasa.2011.tm10155},
eprint = {1102.2233},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Liu, Luo - 2011 - A Constrained {\$}l{\$}1 Minimization Approach to Sparse Precision Matrix Estimation.pdf:pdf;:Users/cyg/Dropbox/Documents/Papers/Cai, Liu, Luo - 2011 - A Constrained {\{}{\$}{\}}l{\{}{\$}{\}}1 Minimization Approach to Sparse Precision Matrix Estimation.pdf:pdf},
isbn = {0162-1459{\$}\backslash{\$}n1537-274X},
issn = {0162-1459},
journal = {J. Amer. Stat. Assoc.},
keywords = {covariance matrix,frobenius norm,gaussian graphical model,precision matrix,rate of convergence,spectral norm},
month = {jun},
number = {494},
pages = {594--607},
title = {{A constrained l1 minimization approach to sparse precision matrix estimation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10155},
volume = {106},
year = {2011}
}
@article{Chan1999,
abstract = {We evaluate the performance of models for the covariance structure of stock re- turns, focusing on their use for optimal portfolio selection.We compare the mod- els' forecasts of future covariances and the optimized portfolios' out-of-sample performance.Afewfactors capture the general covariance structure. Portfolio op- timization helps for risk control, and a three-factor model is adequate for selecting the minimum-variance portfolio. Under a tracking error volatility criterion, which is widely used in practice, larger differences emerge across the models. In gen- eral more factors are necessary when the objective is to minimize tracking error volatility.},
author = {Chan, L.},
doi = {10.1093/rfs/12.5.937},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Chan - 1999 - On portfolio optimization forecasting covariances and choosing the risk model.pdf:pdf},
isbn = {0893-9454},
issn = {14657368},
journal = {Rev. Financial Stud.},
number = {5},
pages = {937--974},
title = {{On portfolio optimization: forecasting covariances and choosing the risk model}},
volume = {12},
year = {1999}
}
@book{Demmel1997,
address = {Philadelphia, PA},
author = {Demmel, James W.},
isbn = {978-0898713893},
publisher = {SIAM},
title = {{Applied Numerical Linear Algebra}},
year = {1997}
}
@article{Fan2013,
abstract = {The paper deals with the estimation of a high dimensional covariance with a conditional sparsity structure and fast diverging eigenvalues. By assuming a sparse error covariance matrix in an approximate factor model, we allow for the presence of some cross-sectional correlation even after taking out common but unobservable factors. We introduce the principal orthogonal complement thresholding method ‘POET' to explore such an approximate factor structure with sparsity. The POET-estimator includes the sample covariance matrix, the factorbased covariance matrix, the thresholding estimator and the adaptive thresholding estimator as specific examples.We provide mathematical insights when the factor analysis is approximately the same as the principal component analysis for high dimensional data. The rates of convergence of the sparse residual covariance matrix and the conditional sparse covariance matrix are studied under various norms. It is shown that the effect of estimating the unknown factors vanishes as the dimensionality increases.The uniform rates of convergence for the unobserved factors and their factor loadings are derived.The asymptotic results are also verified by extensive simulation studies. Finally, a real data application on portfolio allocation is presented.},
author = {Fan, Jianqing and Liao, Yuan and Mincheva, Martina},
doi = {10.1111/rssb.12016},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Fan, Liao, Mincheva - 2013 - Large covariance estimation by thresholding principal orthogonal complements.pdf:pdf},
issn = {13697412},
journal = {J. R. Stat. Soc. Series B Stat. Methodol.},
keywords = {Approximate factor model,Cross-sectional correlation,Diverging eigenvalues,High dimensionality,Low rank matrix,Principal components,Sparse matrix,Thresholding,Unknown factors},
mendeley-tags = {Approximate factor model,Cross-sectional correlation,Diverging eigenvalues,High dimensionality,Low rank matrix,Principal components,Sparse matrix,Thresholding,Unknown factors},
month = {sep},
number = {4},
pages = {603--680},
title = {{Large covariance estimation by thresholding principal orthogonal complements}},
url = {http://doi.wiley.com/10.1111/rssb.12016},
volume = {75},
year = {2013}
}
@article{Friedman2008,
abstract = {We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithm--the graphical lasso--that is remarkably fast: It solves a 1000-node problem ( approximately 500,000 parameters) in at most a minute and is 30-4000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen and B{\"{u}}hlmann (2006). We illustrate the method on some cell-signaling data from proteomics.},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1093/biostatistics/kxm045},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Friedman, Hastie, Tibshirani - 2008 - Sparse inverse covariance estimation with the graphical lasso.pdf:pdf},
issn = {1468-4357},
journal = {Biostatistics},
keywords = {Algorithms,Animals,Biometry,Biometry: methods,Data Interpretation,Humans,Models,Neural Networks (Computer),Proteomics,Proteomics: methods,Reference Values,Regression Analysis,Sample Size,Signal Transduction,Statistical,Time Factors},
month = {jul},
number = {3},
pages = {432--441},
pmid = {18079126},
title = {{Sparse inverse covariance estimation with the graphical lasso}},
url = {https://academic.oup.com/biostatistics/article/9/3/432/224260},
volume = {9},
year = {2008}
}
@techreport{Friedman2010,
abstract = {We propose several methods for estimating edge-sparse and node- sparse graphical models based on lasso and grouped lasso penalties. We develop efficient algorithms for fitting these models when the num- bers of nodes and potential edges are large. We compare them to competing methods including the graphical lasso and SPACE (Peng, Wang, Zhou {\&} Zhu 2008). Surprisingly, we find that for edge selec- tion, a simple method based on univariate screening of the elements of the empirical correlation matrix usually performs as well or better than all of the more complex methods proposed here and elsewhere.},
address = {Stanford, CA},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
booktitle = {Technical Report},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Friedman, Hastie, Tibshirani - 2010 - Applications of the lasso and grouped lasso to the estimation of sparse graphical models.pdf:pdf},
institution = {Stanford University},
title = {{Applications of the lasso and grouped lasso to the estimation of sparse graphical models}},
url = {statweb.stanford.edu/~tibs/ftp/ggraph.pdf},
year = {2010}
}
@article{Golub2002,
abstract = {In this paper, we present an inverse free Krylov subspace method for finding some extreme eigenvalues of the symmetric definite generalized eigenvalue problem {\$}Ax = \backslashlambda B x{\$}. The basic method takes a form of inner-outer iterations and involves no inversion of B or any shift-and-invert matrix {\$}A-\backslashlambda{\_}0 B{\$}. A convergence analysis is presented that leads to a preconditioning scheme for accelerating convergence through some equivalent transformations of the eigenvalue problem. Numerical examples are given to illustrate the convergence properties and to demonstrate the competitiveness of the method.},
author = {Golub, Gene H and Ye, Qiang},
doi = {10.1137/S1064827500382579},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Golub, Ye - 2002 - An Inverse Free Preconditioned Krylov Subspace Method for Symmetric Generalized Eigenvalue Problems.pdf:pdf},
isbn = {1064827500},
issn = {1064-8275},
journal = {SIAM J. Sci. Comput.},
keywords = {1,65f15,ams subject classifications,eigenvalue problems,introduction,iterative methods such as,krylov subspace,pii,preconditioning,s1064827500382579,the,the lanczos algorithm and},
number = {1},
pages = {312--334},
title = {{An inverse free preconditioned Krylov subspace method for symmetric generalized eigenvalue problems}},
url = {https://doi.org/10.1137/S1064827500382579},
volume = {24},
year = {2002}
}
@book{Golub2012,
address = {Baltimore, MD},
author = {Golub, Gene H. and {Van Loan}, Charles F.},
edition = {Fourth},
publisher = {Johns Hopkins University Press},
title = {{Matrix Computations}},
year = {2012}
}
@article{Jagannathan2003,
abstract = {Green and Hollifield (1992) argue that the presence of a dominant factor would result in extreme negative weights in mean-variance efficient portfolios even in the absence of estimation errors. In that case, imposing no-short-sale constraints should hurt, whereas empirical evidence is often to the contrary. We reconcile this apparent contradiction. We explain why constraining portfolio weights to be nonnegative can reduce the risk in estimated optimal portfolios even when the constraints are wrong. Surprisingly, with no-short-sale constraints in place, the sample covariance matrix performs as well as covariance matrix estimates based on factor models, shrinkage estimators, and daily data. Copyright (c) 2003 by the American Finance Association.},
author = {Jagannathan, Ravi and Ma, Tongshu},
doi = {10.1111/1540-6261.00580},
isbn = {00221082},
issn = {00221082},
journal = {J. Finance},
number = {4},
pages = {1651--1683},
title = {{Risk reduction in large portfolios: Why imposing the wrong constraints helps}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1540-6261.00580},
volume = {58},
year = {2003}
}
@article{Khare2015,
abstract = {Sparse high dimensional graphical model selection is a topic of much interest in modern day statistics. A popular approach is to apply l1-penalties to either (1) parametric likelihoods, or, (2) regularized regression/pseudo-likelihoods, with the latter having the distinct advantage that they do not explicitly assume Gaussianity. As none of the popular methods proposed for solving pseudo-likelihood based objective functions have provable convergence guarantees, it is not clear if corresponding estimators exist or are even computable, or if they actually yield correct partial correlation graphs. This paper proposes a new pseudo-likelihood based graphical model selection method that aims to overcome some of the shortcomings of current methods, but at the same time retain all their respective strengths. In particular, we introduce a novel framework that leads to a convex formulation of the partial covariance regression graph problem, resulting in an objective function comprised of quadratic forms. The objective is then optimized via a coordinate-wise approach. The specific functional form of the objective function facilitates rigorous convergence analysis leading to convergence guarantees; an important property that cannot be established using standard results, when the dimension is larger than the sample size, as is often the case in high dimensional applications. These convergence guarantees ensure that estimators are well-defined under very general conditions, and are always computable. In addition, the approach yields estimators that have good large sample properties and also respect symmetry. Furthermore, application to simulated/real data, timing comparisons and numerical convergence is demonstrated. We also present a novel unifying framework that places all graphical pseudo-likelihood methods as special cases of a more general formulation, leading to important insights.},
archivePrefix = {arXiv},
arxivId = {1307.5381},
author = {Khare, Kshitij and Oh, Sang-Yun and Rajaratnam, Bala},
doi = {10.1111/rssb.12088},
eprint = {1307.5381},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Khare, Oh, Rajaratnam - 2015 - A convex pseudolikelihood framework for high dimensional partial correlation estimation with convergence.pdf:pdf;:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Khare, Oh, Rajaratnam - 2015 - A convex pseudolikelihood framework for high dimensional partial correlation estimation with convergen(2).pdf:pdf},
issn = {13697412},
journal = {J. R. Stat. Soc. Series B Stat. Methodol.},
month = {sep},
number = {4},
pages = {803--825},
title = {{A convex pseudolikelihood framework for high dimensional partial correlation estimation with convergence guarantees}},
url = {http://doi.wiley.com/10.1111/rssb.12088},
volume = {77},
year = {2015}
}
@article{Kwon2016,
abstract = {Generalized estimating equations (GEE) proposed by Liang and Zeger (1986) yield a consistent estimator for the regression parameter without correctly specifying the correlation structure of the repeatedly measured outcomes. It is well known that the efficiency of regression coefficient estimator increases with correctly specified working correlation and thus unstructured correlation could be a good candidate. However, lack of positive-definiteness of the estimated correlation matrix in unbalanced case causes practitioners to choose independent, autoregressive or exchangeable matrices as working correlation structure. Our goal is to broaden practical choices of working correlation structure to unstructured correlation matrix or any other matrices by proposing a GEE with a stabilized working correlation matrix via linear shrinkage method in which the minimum eigenvalue is forced to be bounded below by a small positive number. We show that the resulting regression estimator of GEE is asymptotically equivalent to that of the original GEE. Simulation studies show that the proposed modification can stabilize the variance of the GEE regression estimator with unstructured working correlation, and improve efficiency over popular choices of working correlation. Two real data examples are presented where the standard error of the regression coefficient estimator can be reduced using the proposed method.},
author = {Kwon, Yongchan and Choi, Young-Geun and Park, Taesung and Ziegler, Andreas and Paik, Myunghee Cho},
doi = {10.1016/j.csda.2016.08.016},
file = {:Users/cyg/Dropbox/Documents/Papers/mine/Kwon et al. - 2017 - Generalized estimating equations with stabilized working correlation structure.pdf:pdf},
issn = {01679473},
journal = {Comput. Stat. Data Anal.},
keywords = {generalized estimating equations},
month = {feb},
pages = {1--11},
publisher = {Elsevier B.V.},
title = {{Generalized estimating equations with stabilized working correlation structure}},
url = {http://dx.doi.org/10.1016/j.csda.2016.08.016},
volume = {106},
year = {2017}
}
@article{Lam2009,
abstract = {This paper studies the sparsistency and rates of convergence for estimating sparse covariance and precision matrices based on penalized likelihood with nonconvex penalty functions. Here, sparsistency refers to the property that all parameters that are zero are actually estimated as zero with probability tending to one. Depending on the case of applications, sparsity priori may occur on the covariance matrix, its inverse or its Cholesky decomposition. We study these three sparsity exploration problems under a unified framework with a general penalty function. We show that the rates of convergence for these problems under the Frobenius norm are of order (s(n) log p(n)/n)(1/2), where s(n) is the number of nonzero elements, p(n) is the size of the covariance matrix and n is the sample size. This explicitly spells out the contribution of high-dimensionality is merely of a logarithmic factor. The conditions on the rate with which the tuning parameter $\lambda$(n) goes to 0 have been made explicit and compared under different penalties. As a result, for the L(1)-penalty, to guarantee the sparsistency and optimal rate of convergence, the number of nonzero elements should be small: sn'=O(pn) at most, among O(pn2) parameters, for estimating sparse covariance or correlation matrix, sparse precision or inverse correlation matrix or sparse Cholesky factor, where sn' is the number of the nonzero elements on the off-diagonal entries. On the other hand, using the SCAD or hard-thresholding penalty functions, there is no such a restriction.},
author = {Lam, Clifford and Fan, Jianqing},
doi = {10.1214/09-AOS720},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Lam, Fan - 2009 - Sparsistency and Rates of Convergence in Large Covariance Matrix Estimation.pdf:pdf},
issn = {0090-5364},
journal = {Ann. Stat.},
keywords = {Covariance matrix,asymptotic normality,consistency,high-dimensionality,likelihood,nonconcave penalized,sparsistency},
month = {jan},
number = {6B},
pages = {4254--4278},
pmid = {21132082},
title = {{Sparsistency and rates of convergence in large covariance matrix estimation}},
url = {https://projecteuclid.org/euclid.aos/1256303543},
volume = {37},
year = {2009}
}
@article{Lanckriet2002,
abstract = {When constructing a classifier, the probability of correct classification of future data points should be maximized. We consider a binary classification problem where the mean and covariance matrix of each class are assumed to be known. No further assumptions are made with respect to the class- conditional distributions. Misclassification probabilities are then controlled in a worst-case setting: that is, under all possible choices of class-conditional densities with given mean and covariance matrix, we minimize the worst-case (maximum) probability of misclassification of future data points. For a linear decision boundary, this desideratum is translated in a very direct way into a (convex) second order cone optimization problem, with complexity similar to a support vector machine problem. The minimax problem can be interpreted geometrically as minimizing the maximum of the Mahalanobis distances to the two classes. We address the issue of robustness with respect to estimation errors (in the means and covariances of the classes) via a simple modification of the input data. We also show how to exploit Mercer kernels in this setting to obtain nonlinear decision boundaries, yielding a classifier which proves to be competitive with current methods, including support vector machines. An important feature of this method is that a worst-case bound on the probability of misclassification of future data is always obtained explicitly},
author = {Lanckriet, Gert R.G. and {El Ghaoui}, Laurent and Bhattacharyya, Chiranjib and Jordan, Michael I.},
doi = {10.1162/153244303321897726},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Lanckriet et al. - 2002 - A robust minimax approach to classification.pdf:pdf},
issn = {1532-4435},
journal = {J. Mach. Learn Res.},
keywords = {classification,convex optimization,kernel methods,second order cone programming},
pages = {555--582},
title = {{A robust minimax approach to classification}},
url = {http://www.jmlr.org/papers/volume3/lanckriet02a/lanckriet02a.pdf},
volume = {3},
year = {2002}
}
@article{Ledoit2004,
author = {Ledoit, Olivier and Wolf, Michael},
doi = {10.1016/S0047-259X(03)00096-4},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Ledoit, Wolf - 2004 - A well-conditioned estimator for large-dimensional covariance matrices.pdf:pdf},
issn = {0047259X},
journal = {J. Multivar. Anal.},
month = {feb},
number = {2},
pages = {365--411},
title = {{A well-conditioned estimator for large-dimensional covariance matrices}},
url = {http://www.sciencedirect.com/science/article/pii/S0047259X03000964},
volume = {88},
year = {2004}
}
@article{Lehoucq1996,
abstract = {A deflation procedure is introduced that is designed to improve the convergence of an implicitly restarted Arnoldi iteration for computing a few eigenvalues of a large matrix. As the iteration progresses, the Ritz value approximations of the eigenvalues converge at different rates. A numerically stable scheme is introduced that implicitly deflates the converged approximations from the iteration. We present two forms of implicit deflation. The first, a locking operation, decouples converged Ritz values and associated vectors from the active part of the iteration. The second, a purging operation, removes unwanted but converged Ritz pairs. Convergence of the iteration is improved and a reduction in computational effort is also achieved. The deflation strategies make it possible to compute multiple or clustered eigenvalues with a single vector restart method. A block method is not required. These schemes are analyzed with respect to numerical stability, and computational results are presented.},
author = {Lehoucq, R. B. and Sorensen, D. C.},
doi = {10.1137/S0895479895281484},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Lehoucq, Sorensen - 1996 - Deflation Techniques for an Implicitly Restarted Arnoldi Iteration.pdf:pdf},
issn = {0895-4798},
journal = {SIAM J. Matrix Anal. Appl.},
keywords = {1,65f15,65g05,ams subject classifications,an efficient procedure for,approximat-,arnoldi method,deflation,eigenvalues,implicit restarting,ing a subset of,introduction,lanczos method,large sparse n x,n matrix a,the arnoldi method,the arnoldi method is,the eigensystem of a},
number = {4},
pages = {789--821},
title = {{Deflation techniques for an implicitly restarted arnoldi iteration}},
url = {https://doi.org/10.1137/S0895479895281484},
volume = {17},
year = {1996}
}
@article{Little2009,
author = {Little, M.A. and McSharry, P.E. and Hunter, E.J. and Spielman, J. and Ramig, L.O.},
doi = {10.1109/TBME.2008.2005954},
issn = {0018-9294},
journal = {IEEE Trans. Biomed. Eng.},
month = {apr},
number = {4},
pages = {1015--1022},
title = {{Suitability of dysphonia measurements for telemonitoring of parkinson's disease}},
url = {http://ieeexplore.ieee.org/document/4636708/},
volume = {56},
year = {2009}
}
@article{Liu2014,
abstract = {We propose a new approach for estimating high dimensional positive-definite covariance matrices. Our method extends the generalized thresholding operator by adding an explicit eigenvalue constraint. The estimated covariance matrix simultaneously achieves sparsity and positive definiteness. The estimator is rate optimal in the minimax sense and we develop an efficient iterative soft-thresholding and projection algorithm based on the alternating direction method of multipliers. Empirically, we conduct thorough numerical experiments on simulated data sets as well as real data examples to illustrate the usefulness of our method.},
author = {Liu, Han and Wang, Lie and Zhao, Tuo},
doi = {10.1080/10618600.2013.782818},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Wang, Zhao - 2014 - Sparse Covariance Matrix Estimation With Eigenvalue Constraints.pdf:pdf},
issn = {1061-8600},
journal = {J. Comput. Graph. Stat.},
keywords = {covariance matrix estimation,eigenvalue constraint,high dimensional data,positive-definiteness guarantee},
mendeley-tags = {covariance matrix estimation,eigenvalue constraint,high dimensional data,positive-definiteness guarantee},
month = {apr},
number = {2},
pages = {439--459},
title = {{Sparse covariance matrix estimation with eigenvalue constraints}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.2013.782818},
volume = {23},
year = {2014}
}
@article{Marcenko1967,
author = {Marcenko, V. A. and Pastur, L. A.},
doi = {10.1070/SM1967v001n04ABEH001994},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Marcenko, Pastur - 1967 - Distribution of eigenvalues for some sets of random matrices.pdf:pdf},
isbn = {1064-5616},
issn = {0025-5734},
journal = {Math. USSR-Sbornik},
number = {4},
pages = {457--483},
title = {{Distribution of eigenvalues for some sets of random matrices}},
volume = {1},
year = {1967}
}
@article{Mazumder2012,
abstract = {The graphical lasso [5] is an algorithm for learning the structure in an undirected Gaussian graphical model, using ℓ1 regularization to control the number of zeros in the precision matrix $\Theta$ = $\Sigma$−1 [2, 11]. The R package glasso [5] is popular, fast, and allows one to efficiently build a path of models for different values of the tuning parameter. Convergence of glasso can be tricky; the converged precision matrix might not be the inverse of the estimated covariance, and occasionally it fails to converge with warm starts. In this paper we explain this behavior, and propose new algorithms that appear to outperform glasso. By studying the “normal equations” we see that, glasso is solving the dual of the graphical lasso penalized likelihood, by block coordinate ascent; a result which can also be found in [2]. In this dual, the target of estimation is $\Sigma$, the covariance matrix, rather than the precision matrix $\Theta$. We propose similar primal algorithms p-glasso and dp-glasso, that also operate by block-coordinate descent, where $\Theta$ is the optimization target. We study all of these algorithms, and in particular different approaches to solving their coordinate sub-problems. We conclude that dp-glasso is superior from several points of view},
archivePrefix = {arXiv},
arxivId = {1111.5479},
author = {Mazumder, Rahul and Hastie, Trevor},
doi = {10.1214/12-EJS740},
eprint = {1111.5479},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Mazumder, Hastie - 2012 - The graphical lasso New insights and alternatives.pdf:pdf},
issn = {19357524},
journal = {Electron. J. Stat.},
keywords = {Convex analysis/optimization,Graphical lasso,Positive definite matrices,Precision matrix,Semidefinite programming,Sparse inverse covariance selection,Sparsity},
number = {August},
pages = {2125--2149},
title = {{The graphical lasso: New insights and alternatives}},
url = {https://projecteuclid.org/euclid.ejs/1352470831},
volume = {6},
year = {2012}
}
@article{Meinshausen2006,
abstract = {The pattern of zero entries in the inverse covariance matrix of a multivariate normal distribution corresponds to conditional independence restrictions between variables. Covariance selection aims at estimating those structural zeros from data. We show that neighborhood selection with the Lasso is a computationally attractive alternative to standard covariance selection for sparse high-dimensional graphs. Neighborhood selection estimates the conditional independence restrictions separately for each node in the graph and is hence equivalent to variable selection for Gaussian linear models. We show that the proposed neighborhood selection scheme is consistent for sparse high-dimensional graphs. Consistency hinges on the choice of the penalty parameter. The oracle value for optimal prediction does not lead to a consistent neighborhood estimate. Controlling instead the probability of falsely joining some distinct connectivity components of the graph, consistent estimation for sparse graphs is achieved (with exponential rates), even when the number of variables grows as the number of observations raised to an arbitrary power.},
archivePrefix = {arXiv},
arxivId = {math/0608017},
author = {Meinshausen, Nicolai and B{\"{u}}hlmann, Peter},
doi = {10.1214/009053606000000281},
eprint = {0608017},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Meinshausen, B{\"{u}}hlmann - 2006 - High-dimensional graphs and variable selection with the Lasso.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Ann. Stat.},
keywords = {Covariance selection,Gaussian graphical models,Linear regression,Penalized regression},
number = {3},
pages = {1436--1462},
pmid = {239471300013},
primaryClass = {math},
title = {{High-dimensional graphs and variable selection with the Lasso}},
url = {http://projecteuclid.org/euclid.aos/1152540754},
volume = {34},
year = {2006}
}
@article{Peng2009,
abstract = {In this paper, we propose a computationally efficient approach -space(Sparse PArtial Correlation Estimation)- for selecting non-zero partial correlations under the high-dimension-low-sample-size setting. This method assumes the overall sparsity of the partial correlation matrix and employs sparse regression techniques for model fitting. We illustrate the performance of space by extensive simulation studies. It is shown that space performs well in both non-zero partial correlation selection and the identification of hub variables, and also outperforms two existing methods. We then apply space to a microarray breast cancer data set and identify a set of hub genes which may provide important insights on genetic regulatory networks. Finally, we prove that, under a set of suitable assumptions, the proposed procedure is asymptotically consistent in terms of model selection and parameter estimation.},
author = {Peng, Jie and Wang, Pei and Zhou, Nengfeng and Zhu, Ji},
doi = {10.1198/jasa.2009.0126},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Peng et al. - 2009 - Partial Correlation Estimation by Joint Sparse Regression Models.pdf:pdf;:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Peng et al. - 2009 - Partial Correlation Estimation by Joint Sparse Regression Models(2).pdf:pdf},
issn = {0162-1459},
journal = {J. Amer. Stat. Assoc.},
keywords = {concentration network,genetic regulatory network,high-dimension-low-sample-size,lasso,shooting},
month = {jun},
number = {486},
pages = {735--746},
pmid = {19881892},
title = {{Partial correlation estimation by joint sparse regression models}},
volume = {104},
year = {2009}
}
@article{Rothman2012,
abstract = {Using convex optimization, we construct a sparse estimator of the covariance matrix that is positive definite and performs well in high-dimensional settings. A lasso-type penalty is used to encourage sparsity and a logarithmic barrier function is used to enforce positive definiteness. Consistency and convergence rate bounds are established as both the number of variables and sample size diverge. An efficient computational algorithm is developed and the merits of the approach are illustrated with simulations and a speech signal classification example.},
author = {Rothman, Adam J.},
doi = {10.1093/biomet/ass025},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Rothman - 2012 - Positive definite estimators of large covariance matrices.pdf:pdf},
journal = {Biometrika},
keywords = {Barrier function,Classification,Convex optimization,High-dimensional data,Sparsity},
mendeley-tags = {Barrier function,Classification,Convex optimization,High-dimensional data,Sparsity},
number = {3},
pages = {733--740},
title = {{Positive definite estimators of large covariance matrices}},
url = {https://academic.oup.com/biomet/article-abstract/99/3/733/359725},
volume = {99},
year = {2012}
}
@article{Rothman2009,
author = {Rothman, Adam J. and Levina, Elizaveta and Zhu, Ji},
doi = {10.1198/jasa.2009.0101},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Rothman, Levina, Zhu - 2009 - Generalized Thresholding of Large Covariance Matrices.pdf:pdf},
issn = {0162-1459},
journal = {J. Amer. Stat. Assoc.},
month = {mar},
number = {485},
pages = {177--186},
title = {{Generalized thresholding of large covariance matrices}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2009.0101},
volume = {104},
year = {2009}
}
@article{Sorensen1990,
abstract = {The Arnoldi process is a well known technique for approximating a few eigenvalues and corresponding eigenvectors of a general square matrix. Numerical difficulties such as loss of orthogonality and assessment of the numerical quality of the approximations as well as a potential for unbounded growth in storage have limited the applicability of the method. These issues are addressed by fixing the number of steps in the Arnoldi process at a prescribed value k and then treating the residual vector as a function of the initial Arnoldi vector. This starting vector is then updated through an iterative scheme that is designed to force convergence of the residual to zero. The iterative scheme is shown to be a truncation of the standard implicitly shifted QR-iteration for dense problems and it avoids the need to explicitly restart the Arnoldi sequence. The main emphasis of this paper is on the derivation and analysis of this scheme. However, there are obvious ways to exploit parallelism through the matrix-vector operations that comprise the majority of the work in the algorithm. Preliminary computational results are given for a few problems on some parallel and vector computers.},
author = {Sorensen, D. C.},
doi = {10.1137/0613025},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Sorensen - 1990 - Implicit application of polynomial filters in a k-step Arnoldi method.pdf:pdf},
issn = {0895-4798},
journal = {SIAM J. Matrix Anal. Appl.},
keywords = {arnoldi method,eigenvalues,iterative refinement,parallel com-,polynomial filter},
number = {1},
pages = {357--385},
title = {{Implicit application of polynomial filters in a k-step Arnoldi method}},
url = {https://epubs.siam.org/doi/10.1137/0613025},
volume = {13},
year = {1990}
}
@article{Sun2010,
abstract = {This paper considers feature selection for data classification in the presence of a huge number of irrelevant features. We propose a new feature-selection algorithm that addresses several major issues with prior work, including problems with algorithm implementation, computational complexity, and solution accuracy. The key idea is to decompose an arbitrarily complex nonlinear problem into a set of locally linear ones through local learning, and then learn feature relevance globally within the large margin framework. The proposed algorithm is based on well-established machine learning and numerical analysis techniques, without making any assumptions about the underlying data distribution. It is capable of processing many thousands of features within minutes on a personal computer while maintaining a very high accuracy that is nearly insensitive to a growing number of irrelevant features. Theoretical analyses of the algorithm's sample complexity suggest that the algorithm has a logarithmical sample complexity with respect to the number of features. Experiments on 11 synthetic and real-world data sets demonstrate the viability of our formulation of the feature-selection problem for supervised learning and the effectiveness of our algorithm.},
author = {Sun, Yijun and Todorovic, Sinisa and Goodison, Steve},
doi = {10.1109/TPAMI.2009.190},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Sun, Todorovic, Goodison - 2010 - Local-learning-based feature selection for high-dimensional data analysis.pdf:pdf},
isbn = {1939-3539 (Electronic)$\backslash$n0098-5589 (Linking)},
issn = {1939-3539},
journal = {IEEE Trans. Pattern. Anal. Mach. Intell.},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Computer Simulation,Decision Support Techniques,Models,Pattern Recognition,Theoretical},
number = {9},
pages = {1610--1626},
pmid = {20634556},
title = {{Local-learning-based feature selection for high-dimensional data analysis}},
url = {https://dx.doi.org/10.1109/TPAMI.2009.190},
volume = {32},
year = {2010}
}
@article{Tsanas2014,
abstract = {Vocal performance degradation is a common symptom for the vast majority of Parkinson's disease (PD) subjects, who typically follow personalized one-to-one periodic rehabilitation meetings with speech experts over a long-term period. Recently, a novel computer program called Lee Silverman voice treatment (LSVT) Companion was developed to allow PD subjects to independently progress through a rehabilitative treatment session. This study is part of the assessment of the LSVT Companion, aiming to investigate the potential of using sustained vowel phonations towards objectively and automatically replicating the speech experts' assessments of PD subjects' voices as “acceptable” (a clinician would allow persisting during in-person rehabilitation treatment) or “unacceptable” (a clinician would not allow persisting during in-person rehabilitation treatment). We characterize each of the 156 sustained vowel /a/ phonations with 309 dysphonia measures, select a parsimonious subset using a robust feature selection algorithm, and automatically distinguish the two cohorts (acceptable versus unacceptable) with about 90{\%} overall accuracy. Moreover, we illustrate the potential of the proposed methodology as a probabilistic decision support tool to speech experts to assess a phonation as “acceptable” or “unacceptable.” We envisage the findings of this study being a first step towards improving the effectiveness of an automated rehabilitative speech assessment tool.},
author = {Tsanas, Athanasios and Little, Max a. and Fox, Cynthia and Ramig, Lorraine O.},
doi = {10.1109/TNSRE.2013.2293575},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Tsanas et al. - 2014 - Objective automatic assessment of rehabilitative speech treatment in Parkinson's disease.pdf:pdf},
issn = {15344320},
journal = {IEEE Trans. Neural. Syst. Rehabil. Eng.},
keywords = {Decision support tool,Lee Silverman voice treatment (LSVT),Parkinson's disease (PD),Speech rehabilitation},
number = {1},
pages = {181--190},
title = {{Objective automatic assessment of rehabilitative speech treatment in Parkinson's disease}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6678640},
volume = {22},
year = {2014}
}
@article{Witten2011,
abstract = {We consider the graphical lasso formulation for estimating a Gaussian graphical model in the high-dimensional setting. This approach entails estimating the inverse covariance matrix under a multivariate normal model by maximizing the 1-penalized log-likelihood. We present a very simple necessary and sufficient condition that can be used to identify the connected components in the graphical lasso solution. The condition can be employed to determine whether the estimated inverse covariance matrix will be block diagonal, and if so, then to identify the blocks. This in turn can lead to drastic speed improvements, since one can simply apply a standard graphical lasso algorithm to each block separately. Moreover, the necessary and sufficient condition provides insight into the graphical lasso solution: the set of connected nodes at any given tuning parameter value is a superset of the set of connected nodes at any larger tuning parameter value. This article has supplementary material online.},
author = {Witten, Daniela M. and Friedman, Jerome H. and Simon, Noah},
doi = {10.1198/jcgs.2011.11051a},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Witten, Friedman, Simon - 2011 - New Insights and Faster Computations for the Graphical Lasso.pdf:pdf},
issn = {1061-8600},
journal = {J. Comput. Graph. Stat.},
number = {4},
pages = {892--900},
title = {{New insights and faster computations for the graphical lasso}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2011.11051a},
volume = {20},
year = {2011}
}
@article{Won2013,
abstract = {Estimation of high-dimensional covariance matrices is known to be a difficult problem, has many applications, and is of current interest to the larger statistics community. In many applications including so-called the "large p small n" setting, the estimate of the covariance matrix is required to be not only invertible, but also well-conditioned. Although many regularization schemes attempt to do this, none of them address the ill-conditioning problem directly. In this paper, we propose a maximum likelihood approach, with the direct goal of obtaining a well-conditioned estimator. No sparsity assumption on either the covariance matrix or its inverse are are imposed, thus making our procedure more widely applicable. We demonstrate that the proposed regularization scheme is computationally efficient, yields a type of Steinian shrinkage estimator, and has a natural Bayesian interpretation. We investigate the theoretical properties of the regularized covariance estimator comprehensively, including its regularization path, and proceed to develop an approach that adaptively determines the level of regularization that is required. Finally, we demonstrate the performance of the regularized estimator in decision-theoretic comparisons and in the financial portfolio optimization setting. The proposed approach has desirable properties, and can serve as a competitive procedure, especially when the sample size is small and when a well-conditioned estimator is required.},
author = {Won, Joong-Ho and Lim, Johan and Kim, Seung-Jean and Rajaratnam, Bala},
doi = {10.1111/j.1467-9868.2012.01049.x},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Won et al. - 2013 - Condition Number Regularized Covariance Estimation.pdf:pdf},
issn = {1369-7412},
journal = {J. R. Stat. Soc. Series B Stat. Methodol.},
keywords = {Condition number,Convex optimization,Covariance estimation,Cross-validation,Eigenvalue,Portfolio optimization,Regularization,Risk comparisons,Shrinkage},
month = {jun},
number = {3},
pages = {427--450},
pmid = {23730197},
title = {{Condition number regularized covariance estimation}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2012.01049.x/abstract},
volume = {75},
year = {2013}
}
@article{Xue2012,
abstract = {The thresholding covariance estimator has nice asymptotic properties for estimating sparse large covariance matrices, but it often has negative eigenvalues when used in real data analysis. To ﬁx this drawback of thresholding estimation, we develop a positive-deﬁnite 1- penalized covariance estimator for estimating sparse large covariance matrices. We derive an efﬁcient alternating direction method to solve the challenging optimization problem and establish its convergence properties. Under weak regularity conditions, nonasymptotic statistical theory is also established for the proposed estimator. The competitive ﬁnite-sample performance of our proposal is demonstrated by both simulation and real applications.},
author = {Xue, Lingzhou and Ma, Shiqian and Zou, Hui},
doi = {10.1080/01621459.2012.725386},
file = {:Users/cyg/Library/Application Support/Mendeley Desktop/Downloaded/Xue, Ma, Zou - 2012 - Positive-Definite ℓ1 -Penalized Estimation of Large Covariance Matrices.pdf:pdf},
issn = {0162-1459},
journal = {J. Amer. Stat. Assoc.},
keywords = {Alternating direction methods,Matrix norm,Positive-deﬁnite estimation,Soft-thresholding,Sparsity},
mendeley-tags = {Alternating direction methods,Matrix norm,Positive-deﬁnite estimation,Soft-thresholding,Sparsity},
month = {dec},
number = {500},
pages = {1480--1491},
title = {{Positive-definite l1-penalized estimation of large covariance matrices}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2012.725386},
volume = {107},
year = {2012}
}
